{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec871eb9",
   "metadata": {},
   "source": [
    "### Assignment-4\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "Understand and implement model evaluation using cross-validation and improve model performance by hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b546c0c",
   "metadata": {},
   "source": [
    "Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d1297d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Latitude  Longitude        Type     Depth  Magnitude Magnitude Type  \\\n",
      "0  0.583377   0.844368  Earthquake  0.495984   0.277668             MW   \n",
      "1  0.006109   0.698849  Earthquake  0.075272  -0.195082             MW   \n",
      "2 -0.739162  -1.701962  Earthquake -0.413928   0.750418             MW   \n",
      "3 -2.017599  -0.503524  Earthquake -0.454694  -0.195082             MW   \n",
      "4  0.340688   0.691479  Earthquake -0.454694  -0.195082             MW   \n",
      "\n",
      "   Root Mean Square  Source     Status      Year  ...  Source_ISCGEM  \\\n",
      "0         -0.103839  ISCGEM  Automatic -1.915523  ...            1.0   \n",
      "1         -0.103839  ISCGEM  Automatic -1.915523  ...            1.0   \n",
      "2         -0.103839  ISCGEM  Automatic -1.915523  ...            1.0   \n",
      "3         -0.103839  ISCGEM  Automatic -1.915523  ...            1.0   \n",
      "4         -0.103839  ISCGEM  Automatic -1.915523  ...            1.0   \n",
      "\n",
      "   Source_ISCGEMSUP  Source_NC  Source_NN  Source_OFFICIAL  Source_PR  \\\n",
      "0               0.0        0.0        0.0              0.0        0.0   \n",
      "1               0.0        0.0        0.0              0.0        0.0   \n",
      "2               0.0        0.0        0.0              0.0        0.0   \n",
      "3               0.0        0.0        0.0              0.0        0.0   \n",
      "4               0.0        0.0        0.0              0.0        0.0   \n",
      "\n",
      "   Source_SE  Source_US  Source_UW  Status_Reviewed  \n",
      "0        0.0        0.0        0.0              0.0  \n",
      "1        0.0        0.0        0.0              0.0  \n",
      "2        0.0        0.0        0.0              0.0  \n",
      "3        0.0        0.0        0.0              0.0  \n",
      "4        0.0        0.0        0.0              0.0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23409 entries, 0 to 23408\n",
      "Data columns (total 40 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Latitude                23409 non-null  float64\n",
      " 1   Longitude               23409 non-null  float64\n",
      " 2   Type                    23409 non-null  object \n",
      " 3   Depth                   23409 non-null  float64\n",
      " 4   Magnitude               23409 non-null  float64\n",
      " 5   Magnitude Type          23409 non-null  object \n",
      " 6   Root Mean Square        23409 non-null  float64\n",
      " 7   Source                  23409 non-null  object \n",
      " 8   Status                  23409 non-null  object \n",
      " 9   Year                    23409 non-null  float64\n",
      " 10  Day                     23409 non-null  float64\n",
      " 11  Month_sin               23409 non-null  float64\n",
      " 12  Month_cos               23409 non-null  float64\n",
      " 13  Hour_sin                23409 non-null  float64\n",
      " 14  Hour_cos                23409 non-null  float64\n",
      " 15  Type_Explosion          23409 non-null  float64\n",
      " 16  Type_Nuclear Explosion  23409 non-null  float64\n",
      " 17  Type_Rock Burst         23409 non-null  float64\n",
      " 18  Magnitude Type_MD       23409 non-null  float64\n",
      " 19  Magnitude Type_MH       23409 non-null  float64\n",
      " 20  Magnitude Type_ML       23409 non-null  float64\n",
      " 21  Magnitude Type_MS       23409 non-null  float64\n",
      " 22  Magnitude Type_MW       23409 non-null  float64\n",
      " 23  Magnitude Type_MWB      23409 non-null  float64\n",
      " 24  Magnitude Type_MWC      23409 non-null  float64\n",
      " 25  Magnitude Type_MWR      23409 non-null  float64\n",
      " 26  Magnitude Type_MWW      23409 non-null  float64\n",
      " 27  Source_ATLAS            23409 non-null  float64\n",
      " 28  Source_CI               23409 non-null  float64\n",
      " 29  Source_GCMT             23409 non-null  float64\n",
      " 30  Source_ISCGEM           23409 non-null  float64\n",
      " 31  Source_ISCGEMSUP        23409 non-null  float64\n",
      " 32  Source_NC               23409 non-null  float64\n",
      " 33  Source_NN               23409 non-null  float64\n",
      " 34  Source_OFFICIAL         23409 non-null  float64\n",
      " 35  Source_PR               23409 non-null  float64\n",
      " 36  Source_SE               23409 non-null  float64\n",
      " 37  Source_US               23409 non-null  float64\n",
      " 38  Source_UW               23409 non-null  float64\n",
      " 39  Status_Reviewed         23409 non-null  float64\n",
      "dtypes: float64(36), object(4)\n",
      "memory usage: 7.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/springboardmentor943x/ImpactSense-Intern-project/refs/heads/main/Milestone_2/Week_4/Day_18/preprocessed_earthquake_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "print(data.head())\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d72e2",
   "metadata": {},
   "source": [
    "Step 2: Load Dataset and Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f455ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Latitude', 'Longitude', 'Type', 'Depth', 'Magnitude', 'Magnitude Type', 'Root Mean Square', 'Source', 'Status', 'Year', 'Day', 'Month_sin', 'Month_cos', 'Hour_sin', 'Hour_cos', 'Type_Explosion', 'Type_Nuclear Explosion', 'Type_Rock Burst', 'Magnitude Type_MD', 'Magnitude Type_MH', 'Magnitude Type_ML', 'Magnitude Type_MS', 'Magnitude Type_MW', 'Magnitude Type_MWB', 'Magnitude Type_MWC', 'Magnitude Type_MWR', 'Magnitude Type_MWW', 'Source_ATLAS', 'Source_CI', 'Source_GCMT', 'Source_ISCGEM', 'Source_ISCGEMSUP', 'Source_NC', 'Source_NN', 'Source_OFFICIAL', 'Source_PR', 'Source_SE', 'Source_US', 'Source_UW', 'Status_Reviewed']\n",
      "   Latitude  Longitude        Type     Depth  Magnitude Magnitude Type  \\\n",
      "0  0.583377   0.844368  Earthquake  0.495984   0.277668             MW   \n",
      "1  0.006109   0.698849  Earthquake  0.075272  -0.195082             MW   \n",
      "2 -0.739162  -1.701962  Earthquake -0.413928   0.750418             MW   \n",
      "3 -2.017599  -0.503524  Earthquake -0.454694  -0.195082             MW   \n",
      "4  0.340688   0.691479  Earthquake -0.454694  -0.195082             MW   \n",
      "\n",
      "   Root Mean Square  Source     Status      Year  ...  Source_ISCGEM  \\\n",
      "0         -0.103839  ISCGEM  Automatic -1.915523  ...            1.0   \n",
      "1         -0.103839  ISCGEM  Automatic -1.915523  ...            1.0   \n",
      "2         -0.103839  ISCGEM  Automatic -1.915523  ...            1.0   \n",
      "3         -0.103839  ISCGEM  Automatic -1.915523  ...            1.0   \n",
      "4         -0.103839  ISCGEM  Automatic -1.915523  ...            1.0   \n",
      "\n",
      "   Source_ISCGEMSUP  Source_NC  Source_NN  Source_OFFICIAL  Source_PR  \\\n",
      "0               0.0        0.0        0.0              0.0        0.0   \n",
      "1               0.0        0.0        0.0              0.0        0.0   \n",
      "2               0.0        0.0        0.0              0.0        0.0   \n",
      "3               0.0        0.0        0.0              0.0        0.0   \n",
      "4               0.0        0.0        0.0              0.0        0.0   \n",
      "\n",
      "   Source_SE  Source_US  Source_UW  Status_Reviewed  \n",
      "0        0.0        0.0        0.0              0.0  \n",
      "1        0.0        0.0        0.0              0.0  \n",
      "2        0.0        0.0        0.0              0.0  \n",
      "3        0.0        0.0        0.0              0.0  \n",
      "4        0.0        0.0        0.0              0.0  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23409 entries, 0 to 23408\n",
      "Data columns (total 40 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   Latitude                23409 non-null  float64\n",
      " 1   Longitude               23409 non-null  float64\n",
      " 2   Type                    23409 non-null  object \n",
      " 3   Depth                   23409 non-null  float64\n",
      " 4   Magnitude               23409 non-null  float64\n",
      " 5   Magnitude Type          23409 non-null  object \n",
      " 6   Root Mean Square        23409 non-null  float64\n",
      " 7   Source                  23409 non-null  object \n",
      " 8   Status                  23409 non-null  object \n",
      " 9   Year                    23409 non-null  float64\n",
      " 10  Day                     23409 non-null  float64\n",
      " 11  Month_sin               23409 non-null  float64\n",
      " 12  Month_cos               23409 non-null  float64\n",
      " 13  Hour_sin                23409 non-null  float64\n",
      " 14  Hour_cos                23409 non-null  float64\n",
      " 15  Type_Explosion          23409 non-null  float64\n",
      " 16  Type_Nuclear Explosion  23409 non-null  float64\n",
      " 17  Type_Rock Burst         23409 non-null  float64\n",
      " 18  Magnitude Type_MD       23409 non-null  float64\n",
      " 19  Magnitude Type_MH       23409 non-null  float64\n",
      " 20  Magnitude Type_ML       23409 non-null  float64\n",
      " 21  Magnitude Type_MS       23409 non-null  float64\n",
      " 22  Magnitude Type_MW       23409 non-null  float64\n",
      " 23  Magnitude Type_MWB      23409 non-null  float64\n",
      " 24  Magnitude Type_MWC      23409 non-null  float64\n",
      " 25  Magnitude Type_MWR      23409 non-null  float64\n",
      " 26  Magnitude Type_MWW      23409 non-null  float64\n",
      " 27  Source_ATLAS            23409 non-null  float64\n",
      " 28  Source_CI               23409 non-null  float64\n",
      " 29  Source_GCMT             23409 non-null  float64\n",
      " 30  Source_ISCGEM           23409 non-null  float64\n",
      " 31  Source_ISCGEMSUP        23409 non-null  float64\n",
      " 32  Source_NC               23409 non-null  float64\n",
      " 33  Source_NN               23409 non-null  float64\n",
      " 34  Source_OFFICIAL         23409 non-null  float64\n",
      " 35  Source_PR               23409 non-null  float64\n",
      " 36  Source_SE               23409 non-null  float64\n",
      " 37  Source_US               23409 non-null  float64\n",
      " 38  Source_UW               23409 non-null  float64\n",
      " 39  Status_Reviewed         23409 non-null  float64\n",
      "dtypes: float64(36), object(4)\n",
      "memory usage: 7.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/springboardmentor943x/ImpactSense-Intern-project/refs/heads/main/Milestone_2/Week_4/Day_18/preprocessed_earthquake_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "\n",
    "print(data.columns.tolist())\n",
    "\n",
    "\n",
    "print(data.head(5))\n",
    "\n",
    "\n",
    "print(data.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd027c",
   "metadata": {},
   "source": [
    "Step 3: Implement Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0df6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target column: Status_Reviewed\n",
      "All columns: ['Latitude', 'Longitude', 'Type', 'Depth', 'Magnitude', 'Magnitude Type', 'Root Mean Square', 'Source', 'Status', 'Year', 'Day', 'Month_sin', 'Month_cos', 'Hour_sin', 'Hour_cos', 'Type_Explosion', 'Type_Nuclear Explosion', 'Type_Rock Burst', 'Magnitude Type_MD', 'Magnitude Type_MH', 'Magnitude Type_ML', 'Magnitude Type_MS', 'Magnitude Type_MW', 'Magnitude Type_MWB', 'Magnitude Type_MWC', 'Magnitude Type_MWR', 'Magnitude Type_MWW', 'Source_ATLAS', 'Source_CI', 'Source_GCMT', 'Source_ISCGEM', 'Source_ISCGEMSUP', 'Source_NC', 'Source_NN', 'Source_OFFICIAL', 'Source_PR', 'Source_SE', 'Source_US', 'Source_UW', 'Status_Reviewed']\n",
      "X shape: (23409, 39) y shape: (23409,)\n",
      "Number of classes: 2 Class distribution:\n",
      " Status_Reviewed\n",
      "1.0    20770\n",
      "0.0     2639\n",
      "Name: count, dtype: int64\n",
      "Numeric columns: ['Latitude', 'Longitude', 'Depth', 'Magnitude', 'Root Mean Square', 'Year', 'Day', 'Month_sin', 'Month_cos', 'Hour_sin', 'Hour_cos', 'Type_Explosion', 'Type_Nuclear Explosion', 'Type_Rock Burst', 'Magnitude Type_MD', 'Magnitude Type_MH', 'Magnitude Type_ML', 'Magnitude Type_MS', 'Magnitude Type_MW', 'Magnitude Type_MWB', 'Magnitude Type_MWC', 'Magnitude Type_MWR', 'Magnitude Type_MWW', 'Source_ATLAS', 'Source_CI', 'Source_GCMT', 'Source_ISCGEM', 'Source_ISCGEMSUP', 'Source_NC', 'Source_NN', 'Source_OFFICIAL', 'Source_PR', 'Source_SE', 'Source_US', 'Source_UW']\n",
      "Categorical columns: ['Type', 'Magnitude Type', 'Source', 'Status']\n",
      "accuracy: 1.0000 ± 0.0000\n",
      "precision_macro: 1.0000 ± 0.0000\n",
      "recall_macro: 1.0000 ± 0.0000\n",
      "f1_macro: 1.0000 ± 0.0000\n",
      "\n",
      "Overall classification report (cross-validated predictions):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     1.0000    1.0000    1.0000      2639\n",
      "         1.0     1.0000    1.0000    1.0000     20770\n",
      "\n",
      "    accuracy                         1.0000     23409\n",
      "   macro avg     1.0000    1.0000    1.0000     23409\n",
      "weighted avg     1.0000    1.0000    1.0000     23409\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 2639     0]\n",
      " [    0 20770]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix)\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/springboardmentor943x/ImpactSense-Intern-project/refs/heads/main/Milestone_2/Week_4/Day_18/preprocessed_earthquake_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "\n",
    "possible_targets = ['target', 'label', 'class', 'quake_class', 'magnitude_class', 'alert_level']\n",
    "cols = data.columns.tolist()\n",
    "\n",
    "target_col = None\n",
    "for t in possible_targets:\n",
    "    if t in cols:\n",
    "        target_col = t\n",
    "        break\n",
    "if target_col is None:\n",
    "    \n",
    "    target_col = cols[-1]\n",
    "\n",
    "print(\"Using target column:\", target_col)\n",
    "print(\"All columns:\", cols)\n",
    "\n",
    "\n",
    "X = data.drop(columns=[target_col])\n",
    "y = data[target_col]\n",
    "\n",
    "\n",
    "if y.dtype != 'object' and len(np.unique(y)) < 20:\n",
    "    \n",
    "    pass\n",
    "else:\n",
    "    \n",
    "    if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "        y = y.astype('category').cat.codes\n",
    "\n",
    "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
    "print(\"Number of classes:\", len(np.unique(y)), \"Class distribution:\\n\", pd.Series(y).value_counts())\n",
    "\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "], remainder='drop')  \n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "\n",
    "\n",
    "cv_results = cross_validate(clf, X, y, cv=cv, scoring=scoring, return_train_score=False, n_jobs=-1)\n",
    "\n",
    "\n",
    "for metric in scoring:\n",
    "    key = f'test_{metric}'\n",
    "    mean = np.mean(cv_results[key])\n",
    "    std = np.std(cv_results[key])\n",
    "    print(f\"{metric}: {mean:.4f} ± {std:.4f}\")\n",
    "\n",
    "\n",
    "y_pred_cv = cross_val_predict(clf, X, y, cv=cv, n_jobs=-1)\n",
    "print(\"\\nOverall classification report (cross-validated predictions):\\n\")\n",
    "print(classification_report(y, y_pred_cv, digits=4))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y, y_pred_cv)\n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b0b5e",
   "metadata": {},
   "source": [
    "Step 4: Hyperparameter Tuning with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20e148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target column: Status_Reviewed\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "GridSearchCV done in 9.75 minutes\n",
      "Best parameters found:\n",
      "{'classifier__max_depth': None, 'classifier__max_features': 'sqrt', 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 100}\n",
      "Best cross-val score (accuracy): 1.0000\n",
      "\n",
      "Test set results\n",
      "Accuracy: 1.0\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0     1.0000    1.0000    1.0000       528\n",
      "         1.0     1.0000    1.0000    1.0000      4154\n",
      "\n",
      "    accuracy                         1.0000      4682\n",
      "   macro avg     1.0000    1.0000    1.0000      4682\n",
      "weighted avg     1.0000    1.0000    1.0000      4682\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 528    0]\n",
      " [   0 4154]]\n",
      "Best model saved to 'best_rf_pipeline.joblib'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfrom scipy.stats import randint\\nparam_dist = {\\n    \\'classifier__n_estimators\\': randint(100, 500),\\n    \\'classifier__max_depth\\': [None] + list(range(5, 51, 5)),\\n    \\'classifier__min_samples_split\\': randint(2, 11),\\n    \\'classifier__min_samples_leaf\\': randint(1, 5),\\n    \\'classifier__max_features\\': [\\'sqrt\\', \\'log2\\', 0.2, 0.5, None]\\n}\\nrandom_search = RandomizedSearchCV(\\n    estimator=pipe,\\n    param_distributions=param_dist,\\n    n_iter=40,            # number of parameter settings sampled\\n    cv=cv,\\n    scoring=\\'accuracy\\',\\n    random_state=42,\\n    n_jobs=-1,\\n    verbose=2,\\n    refit=True\\n)\\nrandom_search.fit(X_train, y_train)\\nprint(\"RandomizedSearch best params:\", random_search.best_params_)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/springboardmentor943x/ImpactSense-Intern-project/refs/heads/main/Milestone_2/Week_4/Day_18/preprocessed_earthquake_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "\n",
    "possible_targets = ['target', 'label', 'class', 'quake_class', 'magnitude_class', 'alert_level']\n",
    "cols = data.columns.tolist()\n",
    "target_col = next((t for t in possible_targets if t in cols), None)\n",
    "if target_col is None:\n",
    "    target_col = cols[-1]  \n",
    "print(\"Using target column:\", target_col)\n",
    "\n",
    "X = data.drop(columns=[target_col])\n",
    "y = data[target_col]\n",
    "\n",
    "\n",
    "if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "    y = y.astype('category').cat.codes\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols),\n",
    "    ('cat', categorical_transformer, categorical_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 400],          \n",
    "    'classifier__max_depth': [None, 10, 20, 40],          \n",
    "    'classifier__min_samples_split': [2, 5, 10],          \n",
    "    'classifier__min_samples_leaf': [1, 2, 4],           \n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.5]     \n",
    "}\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',  \n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    refit=True,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "print(f\"GridSearchCV done in {(end-start)/60:.2f} minutes\")\n",
    "\n",
    "print(\"Best parameters found:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best cross-val score (accuracy): {grid_search.best_score_:.4f}\")\n",
    "\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nTest set results\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_test, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_test))\n",
    "\n",
    "\n",
    "joblib.dump(best_model, 'best_rf_pipeline.joblib')\n",
    "print(\"Best model saved to 'best_rf_pipeline.joblib'\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "from scipy.stats import randint\n",
    "param_dist = {\n",
    "    'classifier__n_estimators': randint(100, 500),\n",
    "    'classifier__max_depth': [None] + list(range(5, 51, 5)),\n",
    "    'classifier__min_samples_split': randint(2, 11),\n",
    "    'classifier__min_samples_leaf': randint(1, 5),\n",
    "    'classifier__max_features': ['sqrt', 'log2', 0.2, 0.5, None]\n",
    "}\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=40,            # number of parameter settings sampled\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    ")\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"RandomizedSearch best params:\", random_search.best_params_)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e910ac",
   "metadata": {},
   "source": [
    "Step 5: Evaluate Best Model on Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866dc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target column: Status_Reviewed\n",
      "Feature matrix shape: (23409, 39) Target shape: (23409,)\n",
      "Loading saved model from 'best_rf_pipeline.joblib'...\n",
      "Refitting the best model on the full dataset...\n",
      "Refit completed in 0.69s\n",
      "Final model saved to 'final_best_model_full_dataset.joblib'\n",
      "\n",
      "Generating cross-validated predictions for the full dataset (this gives honest estimates)...\n",
      "\n",
      "=== Cross-Validated Metrics (full dataset) ===\n",
      "Accuracy:        1.0000\n",
      "Precision (macro): 1.0000\n",
      "Recall (macro):    1.0000\n",
      "F1 (macro):        1.0000\n",
      "Precision (weighted): 1.0000\n",
      "Recall (weighted):    1.0000\n",
      "F1 (weighted):        1.0000\n",
      "\n",
      "Classification Report (cross-validated predictions):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     1.0000    1.0000    1.0000      2639\n",
      "         1.0     1.0000    1.0000    1.0000     20770\n",
      "\n",
      "    accuracy                         1.0000     23409\n",
      "   macro avg     1.0000    1.0000    1.0000     23409\n",
      "weighted avg     1.0000    1.0000    1.0000     23409\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      " [[ 2639     0]\n",
      " [    0 20770]]\n",
      "\n",
      "ROC AUC (binary): 1.0000\n",
      "\n",
      "Class distribution (full dataset): Counter({1.0: 20770, 0.0: 2639})\n",
      "\n",
      "Step 5 complete — final model refit on full data and cross-validated evaluation finished.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict, cross_val_score\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             classification_report, confusion_matrix, roc_auc_score, roc_curve)\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/springboardmentor943x/ImpactSense-Intern-project/refs/heads/main/Milestone_2/Week_4/Day_18/preprocessed_earthquake_data.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "\n",
    "possible_targets = ['target', 'label', 'class', 'quake_class', 'magnitude_class', 'alert_level']\n",
    "cols = data.columns.tolist()\n",
    "target_col = next((t for t in possible_targets if t in cols), None)\n",
    "if target_col is None:\n",
    "    target_col = cols[-1]  \n",
    "print(\"Using target column:\", target_col)\n",
    "\n",
    "\n",
    "X = data.drop(columns=[target_col])\n",
    "y_raw = data[target_col]\n",
    "\n",
    "\n",
    "label_mapping = None\n",
    "if y_raw.dtype == 'object' or y_raw.dtype.name == 'category':\n",
    "    y = y_raw.astype('category').cat.codes\n",
    "    label_mapping = dict(enumerate(y_raw.astype('category').cat.categories))\n",
    "else:\n",
    "    y = y_raw.copy()\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape, \"Target shape:\", y.shape)\n",
    "if label_mapping is not None:\n",
    "    print(\"Label mapping (code -> original):\", label_mapping)\n",
    "\n",
    "\n",
    "best_model = None\n",
    "saved_model_path = 'best_rf_pipeline.joblib'\n",
    "\n",
    "if os.path.exists(saved_model_path):\n",
    "    print(f\"Loading saved model from '{saved_model_path}'...\")\n",
    "    best_model = joblib.load(saved_model_path)\n",
    "else:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        print(\"Using grid_search.best_estimator_ from the current session.\")\n",
    "    except Exception:\n",
    "        raise RuntimeError(\n",
    "            \"No saved model found and 'grid_search' not available in memory. \"\n",
    "            \"Run GridSearchCV (Step 4) or place 'best_rf_pipeline.joblib' in the working directory.\"\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Refitting the best model on the full dataset...\")\n",
    "start = time.time()\n",
    "best_model.fit(X, y)\n",
    "end = time.time()\n",
    "print(f\"Refit completed in {(end-start):.2f}s\")\n",
    "\n",
    "\n",
    "final_model_path = 'final_best_model_full_dataset.joblib'\n",
    "joblib.dump(best_model, final_model_path)\n",
    "print(f\"Final model saved to '{final_model_path}'\")\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "print(\"\\nGenerating cross-validated predictions for the full dataset (this gives honest estimates)...\")\n",
    "y_pred_cv = cross_val_predict(best_model, X, y, cv=cv, n_jobs=-1, method='predict')\n",
    "\n",
    "\n",
    "y_proba_cv = None\n",
    "try:\n",
    "    y_proba_cv = cross_val_predict(best_model, X, y, cv=cv, n_jobs=-1, method='predict_proba')\n",
    "except Exception:\n",
    "    \n",
    "    y_proba_cv = None\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y, y_pred_cv)\n",
    "precision_macro = precision_score(y, y_pred_cv, average='macro', zero_division=0)\n",
    "recall_macro = recall_score(y, y_pred_cv, average='macro', zero_division=0)\n",
    "f1_macro = f1_score(y, y_pred_cv, average='macro', zero_division=0)\n",
    "\n",
    "precision_weighted = precision_score(y, y_pred_cv, average='weighted', zero_division=0)\n",
    "recall_weighted = recall_score(y, y_pred_cv, average='weighted', zero_division=0)\n",
    "f1_weighted = f1_score(y, y_pred_cv, average='weighted', zero_division=0)\n",
    "\n",
    "print(\"\\n=== Cross-Validated Metrics (full dataset) ===\")\n",
    "print(f\"Accuracy:        {accuracy:.4f}\")\n",
    "print(f\"Precision (macro): {precision_macro:.4f}\")\n",
    "print(f\"Recall (macro):    {recall_macro:.4f}\")\n",
    "print(f\"F1 (macro):        {f1_macro:.4f}\")\n",
    "print(f\"Precision (weighted): {precision_weighted:.4f}\")\n",
    "print(f\"Recall (weighted):    {recall_weighted:.4f}\")\n",
    "print(f\"F1 (weighted):        {f1_weighted:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report (cross-validated predictions):\\n\")\n",
    "if label_mapping is not None:\n",
    "    \n",
    "    target_names = [label_mapping[i] for i in sorted(label_mapping.keys())]\n",
    "    print(classification_report(y, y_pred_cv, digits=4, target_names=target_names, zero_division=0))\n",
    "else:\n",
    "    print(classification_report(y, y_pred_cv, digits=4, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y, y_pred_cv)\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\\n\", cm)\n",
    "\n",
    "\n",
    "if y_proba_cv is not None:\n",
    "    \n",
    "    try:\n",
    "        if len(np.unique(y)) == 2:\n",
    "           \n",
    "            auc = roc_auc_score(y, y_proba_cv[:, 1])\n",
    "            print(f\"\\nROC AUC (binary): {auc:.4f}\")\n",
    "        else:\n",
    "            auc_macro = roc_auc_score(pd.get_dummies(y), y_proba_cv, average='macro', multi_class='ovr')\n",
    "            print(f\"\\nROC AUC (multiclass, macro): {auc_macro:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute ROC AUC:\", str(e))\n",
    "else:\n",
    "    print(\"\\npredict_proba not available for this classifier/pipeline — skipping ROC AUC.\")\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "print(\"\\nClass distribution (full dataset):\", Counter(y))\n",
    "\n",
    "print(\"\\nStep 5 complete — final model refit on full data and cross-validated evaluation finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
